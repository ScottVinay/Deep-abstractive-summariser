{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras-nlp-summariser.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOC0tPIUMeTs24+CGZem0Ou",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ScottVinay/Deep-abstractive-summariser/blob/master/keras_nlp_summariser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPFolHMm0v5y",
        "colab_type": "text"
      },
      "source": [
        "### Notes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXxC8XC-OYiE",
        "colab_type": "text"
      },
      "source": [
        "To do\n",
        "\n",
        "- Evaluation metric to set the damping hyperparameter\n",
        "\n",
        "- Length Bayes correction\n",
        "\n",
        "- Multi-decoder network - decoding both the headline and the main title\n",
        "\n",
        "- RE unknown tokens. A script to consider _u_ in the output, and replace it with a _u_ from the input. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuT1f4Tz-fpw",
        "colab_type": "text"
      },
      "source": [
        "Problems I have faced and overcome\n",
        "\n",
        "- Whole thing won't fit into memory - tried turning into batches, turned out to be the model is too big. Reduce embedding params\n",
        "\n",
        "- Unknown tokens seem to be too prevalent, and become highly represented in the output.\n",
        "\n",
        "- Sentences finish too quickly, high bias towards end token. Solution, manually dampen the __end__token term. TODO - find a way to set this as a hyperparameter.\n",
        "\n",
        "- Coded in beam searching."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yy9lamOGorJl",
        "colab_type": "text"
      },
      "source": [
        "# Preliminaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTQXK_0qowsV",
        "colab_type": "text"
      },
      "source": [
        "## Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDX13oikK6-7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = '/content/drive/My Drive/Python scripts/Data Science/Summariser/'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbgcM5EjVv_W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "b8568054-a170-4547-fc86-5200507ffe6a"
      },
      "source": [
        "from google.colab import drive, files\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DCrzeatkMGK",
        "colab_type": "text"
      },
      "source": [
        "## Modules and global functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kumn2snY2v3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "#from nltk.corpus import stopwords # Need this?\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer, one_hot\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, TimeDistributed\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "import nltk\n",
        "#nltk.download('stopwords')\n",
        "#nltk.download('punkt')\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "\n",
        "import re\n",
        "import pickle\n",
        "\n",
        "import sys"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXvshHTHPQZh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def progressBar(n, tot):\n",
        "    perc = int(100*n/tot)\n",
        "    milli = round(100*n/tot,1)\n",
        "    print('\\r{}/{} ({}%) [{}{}]'.format(n,tot,milli,u'\\u25A4'*perc, '_'*(100-perc)),end='')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oggaoYBb09uJ",
        "colab_type": "text"
      },
      "source": [
        "# Dataframe manipulation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-U_zw1tjSIB_",
        "colab_type": "text"
      },
      "source": [
        "## Loading the dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_czzXGN74dP_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "load = 1"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NkTRzPpvAjq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%% Load\n",
        "if load==0:\n",
        "    df = pd.read_csv(path+'wikihowSep.csv')\n",
        "\n",
        "    df.drop(columns=['overview','sectionLabel'],inplace=True)\n",
        "\n",
        "    titles_to_drop = [\n",
        "    'How to Be Well Read', # This is full of loads of specific titles\n",
        "    ]\n",
        "\n",
        "    for title in titles_to_drop:\n",
        "        df = df[df['title']!=title]\n",
        "        df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    df = df.dropna().reset_index(drop=True)\n",
        "    df['cleaned'] = 0\n",
        "    start_index = 0\n",
        "\n",
        "if load==1:\n",
        "    df = pd.read_pickle(path+'df_split_transformed_June_01.pkl')\n",
        "    start_index = df['cleaned'].sum()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nb6aHu-MpA1x",
        "colab_type": "text"
      },
      "source": [
        "## Cleaning the dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfUeIDxbY6JT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%% Functions\n",
        "\n",
        "def replaceAbbr(text):\n",
        "    #XX need spaces before and after every word when we do this\n",
        "    abbrs = [\n",
        "    [\"don't\",\"do not\"], [\"can't\", \"can not\"], [\"won't\", \"will not\"], [\"shan't\", \"shall not\"],\n",
        "    [\"dont\",\"do not\"], [\"cant\", \"can not\"], [\"wont\", \"will not\"], [\"shant\", \"shall not\"],\n",
        "     \n",
        "    [\"i'll\", \"i will\"], [\"you'll\", \"you will\"], [\"youll\", \"you will\"],\n",
        "    [\"we'll\", \"we will\"], [\"he'll\", \"he will\"], [\"she'll\", \"she will\"],\n",
        "     \n",
        "    [\"i'm\", \"i am\"], [\"im\", \"i am\"], [\"you're\", \"you are\"], [\"youre\", \"you are\"],\n",
        "    [\"we're\", \"we are\"],\n",
        "     \n",
        "    [\"i've\", \"i have\"], [\"ive\", \"i have\"], [\"you've\", \"you have\"], [\"youve\", \"you have\"],\n",
        "    [\"we've\", \"we have\"]\n",
        "    ]\n",
        "\n",
        "    for ab, full in abbrs:\n",
        "        text = re.sub(\"( {} )\".format(ab), \" {} \".format(full), text)\n",
        "    return text\n",
        "\n",
        "def cleanText(text):\n",
        "    # Include \\n\\n = paragraph?\n",
        "    # Include small and large numbers?\n",
        "    if '__start__' in text or '__end__' in text: return text\n",
        "    text = replaceAbbr(text)\n",
        "    text = re.sub(r'(\\.\\s)', ' __fs__ ', text)\n",
        "    text = re.sub(r'[,]', ' __cm__ ', text)\n",
        "    text = re.sub(r'[/]', ' or ', text)\n",
        "    text = re.sub(r'[\\n-]', ' ', text)\n",
        "    text = re.sub(r'[^a-zA-Z\\d_\\s]', '', text)\n",
        "    text = text.lower()\n",
        "    text = text.strip(' ')\n",
        "    text = '__start__ ' + text + ' __end__'\n",
        "    return text"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsfzAsa_ZM5n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8b375542-58e8-4d9a-c046-e7c0f2bc4952"
      },
      "source": [
        "start_index"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "287501"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "01ZS5o_Z7wXS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "91c81c43-d8c3-4f92-9786-534336a5fe21"
      },
      "source": [
        "#%% Clean\n",
        "\n",
        "cleanLimit = len(df)\n",
        "start_index = df['cleaned'].sum()\n",
        "st = time.time()\n",
        "for irow in range(start_index, cleanLimit):\n",
        "    df.loc[irow, 'text']     = cleanText(df.loc[irow, 'text'])\n",
        "    df.loc[irow, 'headline'] = cleanText(df.loc[irow, 'headline'])\n",
        "    df.loc[irow, 'title']    = cleanText(df.loc[irow, 'title'])\n",
        "    df.loc[irow, 'cleaned'] = 1\n",
        "    if irow%10==0:\n",
        "        progressBar(irow,cleanLimit)\n",
        "    if irow%500==0:\n",
        "        df.to_pickle(path+'df_split_transformed_June_01.pkl')\n",
        "progressBar(cleanLimit,cleanLimit)\n",
        "print('Done')\n",
        "df.to_pickle(path+'df_split_transformed_June_01_complete.pkl')\n",
        "print('Time = ',time.time()-st)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "287880/1387270 (20.8%) [▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤________________________________________________________________________________]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-e347d18bee9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mirow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcleanLimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mirow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mcleanText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mirow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mirow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'headline'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcleanText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mirow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'headline'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mirow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mcleanText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mirow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    669\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_setitem_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[0;34m(self, indexer, value)\u001b[0m\n\u001b[1;32m   1027\u001b[0m                 \u001b[0;31m# scalar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m                     \u001b[0msetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36msetter\u001b[0;34m(item, v)\u001b[0m\n\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m                 \u001b[0;31m# reset the sliced object if unique\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[0;31m# we need an iterable, with a ndim of at least 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2937\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2938\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2940\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2999\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3000\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3001\u001b[0;31m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3003\u001b[0m         \u001b[0;31m# check if we are modifying a copy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3623\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3624\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3625\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mset\u001b[0;34m(self, item, value)\u001b[0m\n\u001b[1;32m   1088\u001b[0m             \u001b[0mblk_locs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblklocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_locs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_store\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1090\u001b[0;31m                 \u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblk_locs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_getitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_locs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1091\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m                 \u001b[0munfit_mgr_locs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mblk_locs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mset\u001b[0;34m(self, locs, values)\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \"\"\"\n\u001b[0;32m--> 376\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlocs\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mUb12USpVb0",
        "colab_type": "text"
      },
      "source": [
        "# Dataset creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byUpbjabpeek",
        "colab_type": "text"
      },
      "source": [
        "## Find text and headline lengths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5Ze5XnuqmyV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "2542a7a8-7e1e-4b3a-fbc5-2df383aec7f0"
      },
      "source": [
        "#%% Examine lengths\n",
        "\n",
        "df = df[:start_index-1] # Just train on cleaned samples\n",
        "\n",
        "textlens = []\n",
        "headlens = []\n",
        "\n",
        "for irow in range(len(df)):\n",
        "    if irow%1000==0: progressBar(irow+1, len(df))\n",
        "    textlens.append(len(df.loc[irow,'text'].split()))\n",
        "    headlens.append(len(df.loc[irow,'headline'].split()))\n",
        "progressBar(len(df), len(df))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1387270/1387270 (100.0%) [▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCpxaok_Iq75",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "outputId": "d67d93c0-ce0b-4259-e8c5-2aaeda80cd08"
      },
      "source": [
        "plt.hist(textlens, bins=np.arange(0,500,10)); plt.title('Text length'); plt.show();\n",
        "plt.hist(headlens, bins=np.arange(0,50,1)); plt.title('Headline length'); plt.show();"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0hk98jb58Zj",
        "colab_type": "text"
      },
      "source": [
        "## Train/test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ar7TK3GJICJv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# From the above histograms, we determine:\n",
        "max_text_len = 300\n",
        "max_head_len = 30"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqfrcyfvj99B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%%\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "(xtr,\n",
        " xcv,\n",
        " ytr,\n",
        " ycv) = train_test_split(\n",
        "         df['text'],\n",
        "         df['headline'],\n",
        "         test_size=0.05,\n",
        "         random_state=0,\n",
        "         shuffle=True)\n",
        "\n",
        "#%%"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPTkK1ALphFo",
        "colab_type": "text"
      },
      "source": [
        "## Tokenisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxGIFCwpkBqs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df[:start_index-1] # in case the earler cell was skipped\n",
        "\n",
        "# Without setting a max num_words, there are many words in the vocabulary. \n",
        "# This results in massive embedding layers (since they need to map from R2500\n",
        "# to something smaller.)\n",
        "tokenizer = Tokenizer(\n",
        "    num_words=2000,\n",
        "    oov_token = None)#'__unknown__')\n",
        "\n",
        "# We want to set it so that the tokenizer does NOT remove _\n",
        "index__ = tokenizer.filters.index('_')\n",
        "new_filter = tokenizer.filters[:index__] + tokenizer.filters[index__ + 1:]\n",
        "tokenizer.filters = new_filter\n",
        "\n",
        "tokenizer.fit_on_texts(list(xtr) + list(ytr))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Pgv4vsaZCBw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xtr_tok = tokenizer.texts_to_sequences(xtr)\n",
        "xcv_tok = tokenizer.texts_to_sequences(xcv)\n",
        "\n",
        "ytr_tok = tokenizer.texts_to_sequences(ytr)\n",
        "ycv_tok = tokenizer.texts_to_sequences(ycv)\n",
        "\n",
        "xtr_tok = pad_sequences(xtr_tok, maxlen=max_text_len, padding='post')\n",
        "xcv_tok = pad_sequences(xcv_tok, maxlen=max_text_len, padding='post')\n",
        "\n",
        "# Convert list of arrays into 2D array\n",
        "xtr_tok = np.stack(xtr_tok)\n",
        "xcv_tok = np.stack(xcv_tok)\n",
        "\n",
        "ytr_tok = pad_sequences(ytr_tok, maxlen=max_head_len, padding='post')\n",
        "ycv_tok = pad_sequences(ycv_tok, maxlen=max_head_len, padding='post')\n",
        "\n",
        "#TODO Do we need to stack yxx_tok too?\n",
        "\n",
        "vocab_size = len(tokenizer.word_index)+1\n",
        "\n",
        "# Here I've changed it. Above is what is given in the tutorial, below is\n",
        "# the new one. word_index is hundreds of thousands. It is the pure index\n",
        "# of each word. However, these are cut-off at num_words. The +1 is for \n",
        "# the special token for unknown words\n",
        "vocab_size = tokenizer.num_words + 1\n",
        "\n",
        "end_tok = tokenizer.texts_to_sequences(['__end__'])[0][0]\n",
        "\n",
        "def set_last_to_end(tok_list, end_tok):\n",
        "    for i in range(len(tok_list)):\n",
        "        if tok_list[i][-1] not in [0, end_tok]:\n",
        "            tok_list[i][-1] = end_tok\n",
        "    return tok_list\n",
        "\n",
        "xtr_tok = set_last_to_end(xtr_tok,end_tok)\n",
        "ytr_tok = set_last_to_end(ytr_tok,end_tok)\n",
        "\n",
        "getword  = tokenizer.index_word\n",
        "getindex = tokenizer.word_index"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kocXt4caplCk",
        "colab_type": "text"
      },
      "source": [
        "## Creating objects for model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZjCgIIDpNtx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_model_data(xtok,ytok):\n",
        "    a = [ xtok, ytok[:,:-1] ] # For enc_in, dec_in\n",
        "    b = ytok.reshape(ytok.shape[0], ytok.shape[1], 1)[:,1:]\n",
        "    return a, b"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YK2BEXfpRWL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xtr_tok_modin, ytr_tok_modin = get_model_data(\n",
        "            xtr_tok,\n",
        "            ytr_tok\n",
        "            )\n",
        "        \n",
        "xcv_tok_modin, ycv_tok_modin = get_model_data(\n",
        "            xcv_tok,\n",
        "            ycv_tok\n",
        "            )"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eke66zLP6A5D",
        "colab_type": "text"
      },
      "source": [
        "# Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaIiOyvhpy_6",
        "colab_type": "text"
      },
      "source": [
        "## Model creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYWS_InXZGru",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%% Create the model\n",
        "\n",
        "# Prefix L_ = layer instance\n",
        "# Prefix I_ = special input layer instance\n",
        "\n",
        "latent_dim_1 = 200\n",
        "latent_dim_2 = 200\n",
        "\n",
        "### Encoding layers\n",
        "\n",
        "I_enc_in = Input(name='Encoder_input',shape=(max_text_len,))\n",
        "\n",
        "L_enc_embed = Embedding(\n",
        "        name='Encoder_embedding',\n",
        "        input_dim = vocab_size,\n",
        "        output_dim = latent_dim_1,\n",
        "        trainable = True,\n",
        "        input_length = max_text_len,\n",
        "        ) #TODO include masking?\n",
        "\n",
        "L_enc_lstm = LSTM(\n",
        "    latent_dim_2,\n",
        "    return_sequences=False, #TODO true for Attention\n",
        "    return_state=True,\n",
        "    name='Encoder_LSTM'\n",
        "    )\n",
        "\n",
        "### Encoding connections\n",
        "\n",
        "enc_embedded = L_enc_embed(I_enc_in)\n",
        "enc_out, state_h, state_c = L_enc_lstm(enc_embedded)\n",
        "# If return_sequences = True, then enc_out = state_h?\n",
        "\n",
        "### Decoding layers\n",
        "\n",
        "#I_dec_in = Input(shape = (max_head_len - 1,))\n",
        "I_dec_in = Input(name='dec_ins',shape = (None,))\n",
        "\n",
        "L_dec_embed = Embedding(\n",
        "    vocab_size,\n",
        "    latent_dim_1,\n",
        "    input_length = max_head_len - 1 ,\n",
        "    trainable=True,\n",
        "    name='Dec_embedding'\n",
        "    )\n",
        "\n",
        "L_dec_lstm = LSTM(\n",
        "                  latent_dim_2,\n",
        "                  return_sequences=True,\n",
        "                  return_state=True,\n",
        "                  name='Dec_lstm',\n",
        "                  )\n",
        "\n",
        "#TODO TimeDistributed = Necessary? Learn about this!\n",
        "L_dec_dense = TimeDistributed(\n",
        "                              Dense(vocab_size, activation = 'softmax'),\n",
        "                              name='Time_Dist',\n",
        "                              )\n",
        "\n",
        "### Decoding connections\n",
        "\n",
        "dec_embedded = L_dec_embed(I_dec_in)\n",
        "\n",
        "dec_lstm_out, _, _  = L_dec_lstm(\n",
        "                                dec_embedded,\n",
        "                                initial_state=[state_h, state_c],\n",
        "                                )\n",
        "\n",
        "dec_out = L_dec_dense(dec_lstm_out)\n",
        "\n",
        "###\n",
        "\n",
        "model = Model([I_enc_in, I_dec_in], dec_out)\n",
        "model.compile(optimizer = 'adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
        "# Sparse is not written in tuts, but seems to be required"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dau1JmTQZJj0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Checkpoints \n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
        "\n",
        "savepath = path + 'weights_03_smalltok.hdf5'\n",
        "checkpoint = ModelCheckpoint(savepath, monitor='loss', verbose=1, save_best_only=True, mode='min')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9a3RTcnPy0O",
        "colab_type": "text"
      },
      "source": [
        "## Load model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcuZDN4WP2dj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights(path+'weights_03_smalltok.hdf5')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAMEWRxXJxag",
        "colab_type": "text"
      },
      "source": [
        "## Fit model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Te0a6BrntPa8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 664
        },
        "outputId": "b2361010-57c3-4f65-8b24-73a62158adc3"
      },
      "source": [
        "history = model.fit(\n",
        "                xtr_tok_modin,\n",
        "                ytr_tok_modin,\n",
        "\n",
        "                epochs = 12,\n",
        "                callbacks = [es,checkpoint],\n",
        "                batch_size = 64,\n",
        "                validation_data = (xcv_tok_modin, ycv_tok_modin),\n",
        "                )"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/12\n",
            "20593/20593 [==============================] - ETA: 0s - loss: 0.8329 - acc: 0.8497\n",
            "Epoch 00001: loss improved from inf to 0.83294, saving model to /content/drive/My Drive/Python scripts/Data Science/Summariser/weights_03_smalltok.hdf5\n",
            "20593/20593 [==============================] - 905s 44ms/step - loss: 0.8329 - acc: 0.8497 - val_loss: 0.7497 - val_acc: 0.8560\n",
            "Epoch 2/12\n",
            "20592/20593 [============================>.] - ETA: 0s - loss: 0.7299 - acc: 0.8575\n",
            "Epoch 00002: loss improved from 0.83294 to 0.72987, saving model to /content/drive/My Drive/Python scripts/Data Science/Summariser/weights_03_smalltok.hdf5\n",
            "20593/20593 [==============================] - 898s 44ms/step - loss: 0.7299 - acc: 0.8575 - val_loss: 0.7165 - val_acc: 0.8592\n",
            "Epoch 3/12\n",
            "20592/20593 [============================>.] - ETA: 0s - loss: 0.7040 - acc: 0.8601\n",
            "Epoch 00003: loss improved from 0.72987 to 0.70397, saving model to /content/drive/My Drive/Python scripts/Data Science/Summariser/weights_03_smalltok.hdf5\n",
            "20593/20593 [==============================] - 910s 44ms/step - loss: 0.7040 - acc: 0.8601 - val_loss: 0.7024 - val_acc: 0.8607\n",
            "Epoch 4/12\n",
            "20593/20593 [==============================] - ETA: 0s - loss: 0.6897 - acc: 0.8616\n",
            "Epoch 00004: loss improved from 0.70397 to 0.68965, saving model to /content/drive/My Drive/Python scripts/Data Science/Summariser/weights_03_smalltok.hdf5\n",
            "20593/20593 [==============================] - 896s 43ms/step - loss: 0.6897 - acc: 0.8616 - val_loss: 0.6945 - val_acc: 0.8616\n",
            "Epoch 5/12\n",
            " 4443/20593 [=====>........................] - ETA: 11:25 - loss: 0.6773 - acc: 0.8630"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-c87d61b69799>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                 \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                 \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                 \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mxcv_tok_modin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mycv_tok_modin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEfbCMFB6J3f",
        "colab_type": "text"
      },
      "source": [
        "# Translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQEGXsc2qPB7",
        "colab_type": "text"
      },
      "source": [
        "## Create separate translation models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_4_LxAPZLyO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%% Now we make separate encoder and decoder models\n",
        "\n",
        "encoder_model = Model(\n",
        "        inputs = I_enc_in,\n",
        "        outputs = [enc_out, state_h, state_c]\n",
        "        )\n",
        "\n",
        "I_decoder_h = Input(shape=(latent_dim_2,))\n",
        "I_decoder_c = Input(shape=(latent_dim_2,))\n",
        "I_decoder_m = Input(shape=(max_text_len,latent_dim_1))\n",
        "\n",
        "dec_embedded_2 = L_dec_embed(I_dec_in)\n",
        "\n",
        "dec_lstm_out_2, dec_h_2, dec_c_2  = L_dec_lstm(\n",
        "                                        dec_embedded_2,\n",
        "                                        initial_state=[I_decoder_h, I_decoder_c],\n",
        "                                        )\n",
        "\n",
        "dec_out_2 = L_dec_dense(dec_lstm_out_2)\n",
        "\n",
        "decoder_model = Model(\n",
        "        [I_dec_in] + [I_decoder_h, I_decoder_c],\n",
        "        # With attn: [I_dec_in] + [I_decoder_m, I_decoder_h, I_decoder_c],\n",
        "        [dec_out_2] + [dec_h_2, dec_c_2]\n",
        "        )"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-v9kNvwlNpDU",
        "colab_type": "text"
      },
      "source": [
        "## Sampling functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIOwOVMsNs1d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Single path\n",
        "\n",
        "def probSampler(tokens_in):\n",
        "    tokens = tokens_in[1:] # to account for 0 padding\n",
        "    chosen = np.random.choice(len(tokens), 1, p=tokens/sum(tokens))[0]\n",
        "    chosen = chosen + 1 # to account for 0 padding\n",
        "    return chosen\n",
        "\n",
        "def suppressedArgmax(tokens, damping=2):\n",
        "    chosen, _ = beamSuppressedArgmax(tokens, damping=damping, beamwidth=1)\n",
        "    return chosen[0]\n",
        "\n",
        "def basicArgmax(tokens):\n",
        "    chosen, _ = beamSuppressedArgmax(tokens, damping=1, beamwidth=1)\n",
        "    return chosen[0]\n",
        "\n",
        "# Beam samplers\n",
        "\n",
        "def beamSuppressedArgmax(tokens_in, damping=50, beamwidth=3):\n",
        "    tokens = tokens_in[1:] # account for 0 padding\n",
        "    tokens[getindex['__end__'] - 1] /= damping\n",
        "    \n",
        "    # The tokens of the words with the greatest conditional probabilities.\n",
        "    chosenTokens = tokens.argsort()[::-1][:beamwidth]\n",
        "    # The conditional probabilities themselves.\n",
        "    chosenProbs = tokens[chosenTokens]\n",
        "    chosenTokens += 1 # account for 0 padding\n",
        "    return chosenTokens, chosenProbs"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgPccvFFeKON",
        "colab_type": "text"
      },
      "source": [
        "## Translation functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdZBLlooEh0U",
        "colab_type": "text"
      },
      "source": [
        "### General"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLqE6CZcYBR5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%%\n",
        "\n",
        "# Now to translate a sequence\n",
        "\n",
        "def passThroughEncoder(input_text):\n",
        "    split_in = input_text.split(' ')\n",
        "    if split_in[0] != '__start__':\n",
        "        split_in = ['__start__'] + split_in\n",
        "    if split_in[-1] != '__end__':\n",
        "        split_in = split_in + ['__end__']\n",
        "\n",
        "    new_text = ' '.join(split_in)\n",
        "    input_toks = tokenizer.texts_to_sequences([new_text])\n",
        "    input_toks = pad_sequences(np.array(input_toks), max_text_len, padding='post')\n",
        "\n",
        "    e_out, e_h, e_c = encoder_model.predict(input_toks) \n",
        "    return e_out, e_h, e_c\n",
        "\n",
        "def neaten(text):\n",
        "    text = text.replace(' __cm__', ',')\n",
        "    text = text.replace(' __fs__', '.')\n",
        "    text = text.replace('__start__ ', '')\n",
        "    text = text.replace(' __end__', '')\n",
        "    text = text.strip(' ')\n",
        "    text = text.strip('\\n')\n",
        "    sents = text.split('. ')\n",
        "    if len(text)>1:\n",
        "        text = text[0].upper()+text[1:]\n",
        "    text = '. '.join(i.capitalize() for i in sents)\n",
        "    return text"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5jnhvO7EkAB",
        "colab_type": "text"
      },
      "source": [
        "### Greedy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AsIdia7EaeT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(input_text, sampler=np.argmax, damping=1):\n",
        "    e_out, e_h, e_c = passThroughEncoder(input_text)\n",
        "\n",
        "    decoder_input = np.zeros((1,1))\n",
        "    decoder_input[0,0] = getindex['__start__']\n",
        "\n",
        "    output_sequence = []\n",
        "    out_len = 0\n",
        "    stopper = False\n",
        "    while not stopper:\n",
        "\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "                [decoder_input] + [e_h, e_c] \n",
        "                )\n",
        "\n",
        "        # next_token is a number\n",
        "        output_tokens = output_tokens[0,-1,:]\n",
        "        next_token = sampler(output_tokens, damping=damping) \n",
        "        # TODO check if damping is a keyword of sampler\n",
        "\n",
        "        output_sequence.append(getword[next_token])\n",
        "\n",
        "        out_len += 1\n",
        "\n",
        "        if getword[next_token] == '__end__' or out_len >= 2*max_head_len:\n",
        "            break\n",
        "\n",
        "        decoder_input[0, 0] = next_token\n",
        "        e_h, e_c = h, c # why?\n",
        "\n",
        "    return neaten(' '.join(output_sequence))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szGR1FR5EdEg",
        "colab_type": "text"
      },
      "source": [
        "### Beam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rBNCkXSEcg4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def beamTranslate(\n",
        "    input_text,\n",
        "    width=3,\n",
        "    finalWidth=3, # this might NEED to be equal to width, have a think\n",
        "    damping=20,\n",
        "    sampler=beamSuppressedArgmax,\n",
        "    lengthBayes=False\n",
        "    ):\n",
        "\n",
        "    e_out, e_h, e_c = passThroughEncoder(input_text)\n",
        "\n",
        "    decoder_input = np.zeros((1,1))\n",
        "    decoder_input[0,0] = getindex['__start__']\n",
        "\n",
        "    currentBeam = [{\n",
        "        'outseq':['__start__',], # note, this is a list unlike the strong in the other func\n",
        "        'len':0,\n",
        "        'prob':1,\n",
        "        'h':e_h,\n",
        "        'c':e_c,\n",
        "        }]*1 # start with 1\n",
        "\n",
        "    finalSentences = []\n",
        "\n",
        "    stopper = False\n",
        "    while not stopper:\n",
        "\n",
        "        newBeam = []\n",
        "\n",
        "        for cand in currentBeam: # cand = candidate\n",
        "            decoder_input[0,0] = getindex[cand['outseq'][-1]]\n",
        "            # h, c change when a word is passed through the decoder,\n",
        "            # so we need to store these for each possible candidate sentence.\n",
        "            h = cand['h']\n",
        "            c = cand['c']\n",
        "\n",
        "            output_tokens, new_h, new_c = decoder_model.predict(\n",
        "                    [decoder_input] + [h, c] \n",
        "                    )\n",
        "            output_tokens = output_tokens[0,-1,:]\n",
        "\n",
        "            next3tokens, next3condProbs = sampler(output_tokens, damping=damping)\n",
        "\n",
        "            for token, conProb in zip(next3tokens, next3condProbs):\n",
        "                word = getword[token]\n",
        "\n",
        "                if word=='__end__':\n",
        "                    finalSentences.append({\n",
        "                        # ' '.join(...) creates a single sentence from \n",
        "                        # the list. We apply neaten to make it human-readable.\n",
        "                        'sent': neaten(' '.join(cand['outseq'])),\n",
        "                        'prob': cand['prob']*conProb,\n",
        "                        'len': cand['len']+1,\n",
        "                    })\n",
        "\n",
        "                else:\n",
        "                    new_outseq = cand['outseq'].copy()\n",
        "                    new_outseq.append(word)\n",
        "\n",
        "                    newBeam.append({\n",
        "                        'outseq': new_outseq,\n",
        "                        'len': cand['len']+1,\n",
        "                        'prob': cand['prob']*conProb, # = joint prob ya?\n",
        "                        'h': new_h,\n",
        "                        'c': new_c\n",
        "                    })\n",
        "        \n",
        "        if len(finalSentences)>=finalWidth:\n",
        "            stopper = True\n",
        "            break\n",
        "\n",
        "        currentBeam = sorted(newBeam, key=lambda x: x['prob'])[::-1][:width]\n",
        "\n",
        "    finalSentences = sorted(finalSentences, key = lambda x: x['prob'])[::-1][:width]\n",
        "    return finalSentences"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcwlI01DfWx-",
        "colab_type": "text"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-5J4Zb_Exlm",
        "colab_type": "text"
      },
      "source": [
        "### Greedy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wp9-mbpcOs9r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "cb68d7c9-302d-4edc-c913-a6c17ce35ebb"
      },
      "source": [
        "i=np.random.randint(0,7850) # = len xcv\n",
        "\n",
        "print(i)\n",
        "print('\\n')\n",
        "\n",
        "\n",
        "print('Text:')\n",
        "print(neaten(xcv.loc[xcv.index[i]]))\n",
        "print('\\n')\n",
        "\n",
        "print('Actual headline:')\n",
        "print(neaten(ycv.loc[xcv.index[i]]))\n",
        "print('\\n')\n",
        "\n",
        "print('Generated headline:')\n",
        "print( translate(xcv.loc[xcv.index[i]] , suppressedArgmax, damping=10) )"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5685\n",
            "\n",
            "\n",
            "Text:\n",
            "This can make it even more twirly. You can use regular satin ribbon or curling ribbon (like the type you'd use on a balloon). Cut five to seven pieces of ribbon that are a little bit shorter than your dowel and glue them to the top of your dowel.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Actual headline:\n",
            "\n",
            "consider adding some very thin ribbon strands to the top of your wand.\n",
            "\n",
            "\n",
            "Generated headline:\n",
            "Tie a ribbon around the top of the\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNgM_6AXEz4i",
        "colab_type": "text"
      },
      "source": [
        "### Beam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDJUhkQa27u9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "b753ba43-23c2-48cb-daba-35da2294e4d6"
      },
      "source": [
        "i=np.random.randint(0,7850) # = len xcv\n",
        "\n",
        "print(i)\n",
        "print('\\n')\n",
        "\n",
        "\n",
        "print('Text:')\n",
        "print(neaten(xcv.loc[xcv.index[i]]))\n",
        "print('\\n')\n",
        "\n",
        "print('Actual headline:')\n",
        "print(neaten(ycv.loc[xcv.index[i]]))\n",
        "print('\\n')\n",
        "\n",
        "print('Generated headlines:')\n",
        "\n",
        "for io, output in enumerate(beamTranslate(xcv.loc[xcv.index[i]] , damping=20)):\n",
        "    print('{}.'.format(io+1), output['sent'])"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4037\n",
            "\n",
            "\n",
            "Text:\n",
            "You may find that they are supportive of your decision. If they are more traditional, you may encounter some resistance. Just keep in mind that you need not justify your decision to anyone. Hopefully, they will soon respect your plans to keep your full identity.\n",
            "\n",
            "\n",
            "Actual headline:\n",
            "Inform his parents, and yours.\n",
            "\n",
            "\n",
            "Generated headlines:\n",
            "1. Dont be afraid to ask for help\n",
            "2. Dont be afraid to ask for advice\n",
            "3. Dont be afraid to ask for help you\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HP81NTc9IAQE",
        "colab_type": "text"
      },
      "source": [
        "# Best outcomes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TW6idByk5Vb",
        "colab_type": "text"
      },
      "source": [
        "## Greedy outcomes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfMj0bQ3iClG",
        "colab_type": "text"
      },
      "source": [
        "Text:\n",
        "In addition to stiffness and spasms in the neck and jaw muscles,  tetanus also causes abdominal and spinal tightness or cramping,  widespread muscle twitching,  difficulty swallowing,  fever,  sweating and rapid heart rateif you have symptoms of tetanus,  you will need to be treated in a hospital  it is a serious infection that cannot be treated at home.   tetanus symptoms can appear anytime from a few days to several weeks after the bacteria enter your body  often through a puncture wound of the foot,  such as stepping on a contaminated nail. The doctor will rely on a physical exam,  as well as a medical and immunization history,  to diagnose tetanus. No lab or blood tests are helpful to detect tetanus. Diseases that cause similar symptoms to tetanus that your doctor will want to rule out include meningitis,  rabies and strychnine poisoningthe medical staff will need to clean the wound as well,  removing any debris,  dead tissue,  and foreign objects\n",
        "\n",
        "\n",
        "Actual headline:\n",
        "Go to the hospital\n",
        "\n",
        "\n",
        "Generated headline:\n",
        "See a doctor if the pain is severe and severe abdominal pain or swelling in the morning and take a year of pregnancy\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaD4aFrkkBzA",
        "colab_type": "text"
      },
      "source": [
        "Text:\n",
        "Running water will not only clean the wound,  but help to stop the bleeding. Run cold water over the cut to constrict the blood vessels and stop the bleeding. Doing the same with hot water will cauterize the cut,  allowing the blood to clot. Dont use both hot and cold water   just one or the other should do the trick.   you can use an ice cube instead of cold water to close off the arteries. Hold the ice to the cut for a few seconds until the wound closes up and stops bleeding. If you have multiple small cuts on your body,  taking a hot shower will clean off all the blood and cauterize the multiple gashes concurrently\n",
        "\n",
        "\n",
        "Actual headline:\n",
        "Use water\n",
        "\n",
        "\n",
        "Generated headline:\n",
        "Apply a cold to the area of the injury"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axR41aKKxGmy",
        "colab_type": "text"
      },
      "source": [
        "Text:\n",
        "Add a big triangular shape over the top of the circle and then draw a smaller one beside the big triangle. Erase unnecessary angles to get the outline of mordecai’s face.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Actual headline:\n",
        "\n",
        "draw a circle.\n",
        "\n",
        "\n",
        "Generated headline:\n",
        "Draw a circle for the head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHs2-Zmbxnw4",
        "colab_type": "text"
      },
      "source": [
        "Text:\n",
        "If a co worker is having difficulty with his or her work,  offer your assistance,  especially if the problem deals with an area of expertise you feel comfortable with. Doing this shows both a healthy dose of team spirit as well as a broad knowledge base and skill set.   make sure that you do not gloat or put yourself above others after helping them,  though. You need to be helpful and confident,  yet also humble.\n",
        "\n",
        "\n",
        "Actual headline:\n",
        "Help others\n",
        "\n",
        "\n",
        "Generated headline:\n",
        "Be a good friend"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbJYIg-Cy0zD",
        "colab_type": "text"
      },
      "source": [
        "Text:\n",
        "An inventory is a list of all the assets you own that are worth anything. To keep it simple, you should only consider assets that are of relevance; so list cars, houses, motorbikes, bank accounts, cds, 401-ks, etc.; but skip the skirts in your closet, the books in your shelves, or the old tv.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Actual headline:\n",
        "\n",
        "build an inventory in the first column.\n",
        "\n",
        "\n",
        "Generated headline:\n",
        "Make a list of items that are available to you"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YBKQZYzzPaf",
        "colab_type": "text"
      },
      "source": [
        "Text:\n",
        "When you have a cold sore,  you should wash your hands carefully before touching your face and making contact with other people,  especially babies,  as herpes simplex can quickly spread this wayone option is to keep a hand sanitizer or moist wipes with you when heading out of your home or at work so you can keep your hands clean on the go.\n",
        "\n",
        "\n",
        "Actual headline:\n",
        "Wash your hands frequently\n",
        "\n",
        "\n",
        "Generated headline:\n",
        "Wash your hands with soap and water"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRyZ-bsJH11e",
        "colab_type": "text"
      },
      "source": [
        "## Beam outcomes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6G3nS56H620",
        "colab_type": "text"
      },
      "source": [
        "Text:\n",
        "The minimum requirements to become a licensed real estate agent in the state of montana are that you must be at least 18 years of age and have a high school diploma,  or an equivalent,  such as a ged. If you do not have a high school diploma or a an equivalent,  you will need to obtain a ged before completing your pre license educationcommunity colleges offer classes that will help prepare you to pass the ged\n",
        "\n",
        "\n",
        "Actual headline:\n",
        "Obtain your ged\n",
        "\n",
        "\n",
        "Generated headlines:\n",
        "1. Get a license\n",
        "2. Get a degree\n",
        "3. Get a college degree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blephEYwIz-J",
        "colab_type": "text"
      },
      "source": [
        "Text:\n",
        "Stock up well before thanksgiving so that you don't have to do all of your shopping at once. Things like pumpkin pie filling or canned cranberry sauce are more readily available before november and can be purchased at most grocery stores. Things like plastic storage bags, aluminum foil, and sealable storage containers are also great things to pick up in advance.\n",
        "\n",
        "\n",
        "Actual headline:\n",
        "Purchase non-perishable food early.\n",
        "\n",
        "\n",
        "Generated headlines:\n",
        "1. Gather your supplies\n",
        "2. Make a list of items you need\n",
        "3. Make a list of items you want to use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFF5-7npI9q4",
        "colab_type": "text"
      },
      "source": [
        "Text:\n",
        "A large part of being mature and responsible enough to live on your own is understanding how much money you will have going out,  what youll have coming in,  and how much is left over at the end. A budget can be extremely useful,  because then you know exactly where your money is going,  and whether you can afford certain things or notfor instance,  if you know your budget only allots 40 for food per week,  you will immediately know that you shouldnt spend 10 of that on a single fast food meal.\n",
        "\n",
        "\n",
        "Actual headline:\n",
        "Create a budget\n",
        "\n",
        "\n",
        "Generated headlines:\n",
        "1. Make a budget\n",
        "2. Make a budget for your budget\n",
        "3. Make a list of your budget"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKlDwmyBUFRs",
        "colab_type": "text"
      },
      "source": [
        "Text:\n",
        "Heat and cold can help to relieve pain and muscle tension in your neck and headapply a moist hot towel or warm compress to the back of your neck or on your forehead. You can also take a long,  hot shower,  being sure to run water down your head and on the back of your neck. Wrap an ice pack in a towel and place it on the back of your neck or on your forehead.\n",
        "\n",
        "\n",
        "Actual headline:\n",
        "Apply a hot or cold compress to your head\n",
        "\n",
        "\n",
        "Generated headlines:\n",
        "1. Take a shower\n",
        "2. Take a warm bath\n",
        "3. Take a warm shower"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJG2-dCxk28O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = [\n",
        "     {'a':5,'b':'Paris'},\n",
        "     {'a':1,'b':'London'},\n",
        "     {'a':4,'b':'Madrid'},\n",
        "     ]"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMoP8BAxajT9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f8bfefa9-74db-4228-b3ab-8df53ffe2bcf"
      },
      "source": [
        "sorted(a, key=lambda x: x['a'])[::-1][:2]"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'a': 5, 'b': 'Paris'}, {'a': 4, 'b': 'Madrid'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNYt-Y9Aar1w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "456f0712-884e-41ad-b330-874dcf61f485"
      },
      "source": [
        "a[a.argsort()[::-1][:3]]"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([6. , 5. , 3.4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkpFhnjYa4h-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "21912d18-b526-489d-a54c-2d15498fe07d"
      },
      "source": [
        "neaten(xtr.loc[1])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Depending on what scale you intend to sell your art pieces,  you may want to get an account on an online art community or store,  like deviant art. With 15  20\\xa0 brokerage,  you can also find many online art galleries like art brokerage,  diva art group,  or saatchi art that will show your artwork online and sell them to the buyer in completely secured transaction. There are also many other possible sites,  such as etsy heavier on the crafts but still arty,  ebay auction site,  an amazon store an online store platform,  cafepress for printing your artwork onto stuff like mugs,  craigslist general classifieds,  and quite a lot of other artwork based sales sites do a general search.   read the terms and conditions of every site very carefully. Know what commission or percentage the site takes,  know what protections or lack of them that the site offers,  know what clientele generally peruse the site,  know the general sales brought in by the site,  know everything you can that is relevant to your sales. If your goal is to turn your art into an investment property,  then selling online is likely to be a much longer path to this end. This is largely because it is generally more difficult for unknown artists to secure higher priced sales with serious collectors online than it is through a traditional gallery where such concrete decisions can be made safely. It is best to see selling art online as an adjunct to your usual methods of selling art,  not your only means.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9RxcvZVEhiC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "41b282f4-20cd-4ec9-b417-303b41bf1f3d"
      },
      "source": [
        "neaten(ytr.loc[1])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Join online artist communities'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OG7N4hxTEog9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}